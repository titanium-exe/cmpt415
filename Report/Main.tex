\documentclass[sigconf]{acmart}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\copyrightyear{2021} 
\acmYear{2021} 
\setcopyright{rightsretained} 
\settopmatter{printacmref=true}

\begin{document}
\fancyhead{}
\title{CMPT 415 - Special Research Project}
%\author{An Author}
\author{Ekamleen Maan}
\email{ekm9@sfu.ca}
\affiliation{%
  \institution{School of Computing Science\\ Simon Fraser University}
  \city{Burnaby}
  \state{BC}
  \country{Canada}
}


\begin{abstract}
This project investigates using GPT-4 to generate P4\_16 code from natural language intents for Intent-Based Networking. A closed-loop system with compiler feedback iteratively refines the code, revealing that scope-specific prompts outperform general ones for complex tasks. Results suggest LLMs can automate parts of network configuration, with reliability tied to prompt design.
\end{abstract}



\keywords{Intent Based Networking, LLMs, P4\_16, Prompts, Intent}
\maketitle
% \textbf{Start} by writing a few lines describing some background information
% \cite{ConfPaperName, ConfPaperName}, and how your work relates to the domain.  
\section{Literature Review}

The use of large language models (LLMs) in networking tasks such as configuration synthesis, intent translation, and code analysis has been the focus of several recent studies.

Wang et al.\ introduce \textit{NetConfEval}~\cite{wang2023netconfeval}, a benchmark framework for assessing the ability of LLMs to translate natural language-based network configuration tasks into device-level commands. Their study reveals that while LLMs show considerable potential in understanding high-level intents, they often suffer from syntactic inaccuracies and limited semantic grounding. Our work builds on these findings by introducing a closed-loop system that uses compiler feedback to iteratively refine LLM-generated P4\_16 code, improving syntactic correctness over multiple iterations.

Fang et al.~\cite{fang2024codeanalysis} conduct an empirical evaluation of LLMs on both normal and obfuscated code analysis tasks. Their results indicate that while GPT-4 can understand and explain source code with high accuracy, it struggles with de-obfuscation and consistently producing compilable outputs. Our work aligns with these findings and extends them by evaluating how compiler feedback can improve the syntactic validity of LLM-generated code in a domain-specific language (DSL), namely P4\_16.

Finally, He et al.\ introduce the \textit{NADA} framework~\cite{he2024nada}, which demonstrates that LLMs can be guided to design high-performing network algorithms through component-level prompting and iterative evaluation. Rather than attempting end-to-end code generation, NADA decomposes algorithm design into modular tasks—such as state definition and neural network architecture—which are each handled with dedicated prompts. Inspired by this strategy, our system adopts a scope-based prompting approach to isolate intents by functionality (e.g., parsing, forwarding, header modification). This modular structure improves both the reliability and convergence of GPT-4-generated P4\_16 code, particularly for moderately complex network intents.

\section{Introduction}
In traditional network configuration, operators must write and maintain low level rules and code to control packet behavior, making it error prone and inflexible. Intent Based Networking (IBN) proposes a paradigm shift: users define high level intents such as "block all HTTP traffic" or "forward packets from host A to port X" and the system handles the translation into low level configuration. This approach can reduce complexity, improve reliability, and enable more agile network management.

The growing capabilities of Large Language Models (LLMs), particularly GPT 4, offer a promising path to bridge this intent to configuration gap. In this work, we explore how GPT 4 can generate P4\_16 code, a programming language used for programming data planes, from natural language intents. By embedding the LLM within a feedback loop that uses the compiler (p4c bm2 ss) as a validator, we attempt to iteratively refine the generated code until a syntactically correct program is produced.
 

\section{Problem Definition}
Modern networks require operators to manually write low-level configurations, which is both complex and error-prone. Intent-Based Networking (IBN) aims to simplify this process by allowing users to express high-level goals in natural language. However, automatically translating these goals into correct, compilable P4\_16 programs remains a challenge.

This project explores whether a closed-loop system using large language models (LLMs), specifically GPT-4, can reliably generate and refine P4\_16 code from natural language intents. The goal is to automate the intent-to-code process with compiler feedback guiding iterative improvements.



\section{System Design}

Figure~\ref{fig:system-design}  illustrates the architecture of the closed-loop system used to translate high-level user intent into compilable P4\_16 code using GPT-4. 

The process begins with a user-provided intent in natural language. This intent is first processed by a prompt generator, which formats the input into a detailed instruction suitable for GPT-4. The language model then generates a candidate P4\_16 program based on the prompt.

The generated code is compiled using the \texttt{p4c-bm2-ss} compiler. If the code compiles successfully, it can be used as a working P4 program. If it fails to compile, the system collects the compiler error messages and sends them together with the code back to GPT-4. GPT-4 then tries to fix the errors and generate a corrected version of the code.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{feedback_loop.png}
    \caption{Closed-loop system for intent-to-P4 compilation.}
    \label{fig:system-design}
\end{figure}



\section{Implementation}

The system is implemented using a Python script that automates the entire loop of intent translation, code generation, compilation, and correction. The steps are as follows:

\begin{enumerate}
    \item \textbf{User Input:} The user enters a high-level intent in natural language, such as ``Set source MAC to a fixed value and drop packets with destination IP 8.8.8.8''.
    
    \item \textbf{Prompt Generation:} The script transforms this intent into a detailed prompt using a predefined template designed for GPT-4.
    
    \item \textbf{Code Generation:} The prompt is sent to GPT-4 through the OpenAI API. The model responds with P4\_16 code based on the given intent.
    
    \item \textbf{Compilation:} The generated code is saved to a file and compiled using \texttt{p4c-bm2-ss}, the reference compiler for the BMv2 \texttt{v1model} architecture. If the compilation succeeds, the process ends and the code is considered deployable.
    
    \item \textbf{Error Feedback:} If the code fails to compile, the script logs the compiler error messages and asks the user whether to attempt automatic correction.
    
    \item \textbf{Fixing Errors with GPT-4:} If the user agrees, the compiler errors along with the original code are sent back to GPT-4. The model attempts to generate a corrected version of the code.
    
    \item \textbf{Retry Compilation:} The corrected code is compiled again. This process continues iteratively until the code compiles or a fixed number of iterations is reached.
    
    \item \textbf{Database Logging:} Each iteration is recorded in a local SQLite database, including the user intent, generated code, compiler output, and a success flag. This helps track progress across iterations.
\end{enumerate}

This closed-loop design allows GPT-4 to improve its output with compiler feedback, enabling more accurate translation from natural language intents to valid P4\_16 programs.

\section{Results}
This section presents the outcomes of seven experiments designed to assess the capability of GPT-4 in generating valid P4\_16 code from natural language intents, targeting the BMv2 \texttt{v1model} architecture and validated with the \texttt{p4c-bm2-ss} compiler.

\subsection*{Experiment Summary}
\begin{itemize}
    \item \textbf{Experiment 1 (Failed)}: A general prompt aimed at parsing Ethernet and IPv4 headers and forwarding packets unchanged failed after five iterations. Only one of at least four persistent errors (e.g., incomplete parser transitions) was resolved, indicating inadequate guidance for multi-header parsing.
    \item \textbf{Experiment 2 (Success)}: Simplifying the intent to parsing only Ethernet headers and forwarding packets, the same general prompt succeeded after five iterations. This suggests basic parsing tasks are more tractable with minimal refinement.
    \item \textbf{Experiment 3 (Success)}: A detailed prompt for dropping TCP packets with destination port 80 produced correct code in three iterations. The specificity of header definitions and control logic enabled rapid convergence to a compilable solution.
    \item \textbf{Experiment 4 (Failed)}: A general prompt failed to implement IPv4 packet counting per ingress port using registers, even after five iterations. Errors like incorrect register indexing persisted, revealing limitations in handling stateful operations without explicit instructions.
    \item \textbf{Experiment 5 (Success)}: A tailored prompt with precise register syntax (e.g., \texttt{register<bit<32>>(256)}) succeeded in three iterations for the same packet counting intent as Experiment 4, despite a minor compiler warning (e.g., unused variable). This highlights the value of scope-specific guidance.
    \item \textbf{Experiment 6 (Success)}: A scoped prompt for header modification intents—rewriting destination IP (10.0.0.1 to 10.0.0.2), NAT translation (192.168.0.100 to 172.16.0.100), and setting a fixed source MAC with dropping for IP 8.8.8.8—succeeded with an average of 4.67 iterations (min: 2, max: 9). This demonstrates flexibility across related tasks with proper scoping.
    \item \textbf{Experiment 7 (Failed)}: A general prompt again failed to implement register-based IPv4 packet counting after ten iterations, with errors such as mismatched bit-widths persisting. This reinforces the need for detailed, intent-specific prompts for advanced functionality.
\end{itemize}

\subsection*{Quantitative Analysis}
The system achieved a success rate of 57.1\% (4 out of 7 experiments succeeded). \\ For successful cases:
\begin{itemize}
    \item Average iterations to success: 4.25 (Experiment 2: 5, Experiment 3: 3, Experiment 5: 3, Experiment 6: 4.67 average across three intents).
    \item Minimum iterations: 2 (Experiment 6, Intent: Set source MAC to a fixed value and drop the packet if the destination IP is 8.8.8.8).
    \item Maximum iterations: 9 (Experiment 6, Intent: Rewrite destination IP to 10.0.0.2 if it is 10.0.0.1).
\end{itemize}
For failed cases:
\begin{itemize}
    \item Average iterations attempted: 6.67 (Experiment 1: 5, Experiment 4: 5, Experiment 7: 10).
    \item Maximum iterations: 10 (Experiment 7), with no success.
\end{itemize}

Notably, iteration counts did not always align with perceived intent complexity. Easier intents like parsing (Experiment 2, 5 iterations) and filtering (Experiment 3, 3 iterations) succeeded within 3–5 iterations, and the register-based intent in Experiment 5 took 3 iterations with a tailored prompt. However, in Experiment 6, the seemingly complex intent of setting a source MAC and dropping packets (2 iterations) converged faster than the simpler IP rewriting intent (9 iterations). Failed cases with complex intents (Experiments 1, 4, 7) averaged higher iterations (6.67), suggesting that while complexity can increase iterations, prompt design and LLM variability also play significant roles in efficiency.

\subsection*{Key Insights}
\begin{itemize}
    \item \textbf{Prompt Specificity Drives Success}: General prompts consistently failed for complex intents (Experiments 1, 4, 7), while scope-specific prompts succeeded (Experiments 3, 5, 6), reducing iterations and errors by providing clear semantic guidance.
    \item \textbf{Task Complexity Impacts Feasibility}: Simple stateless tasks like header parsing (Experiment 2) and TCP filtering (Experiment 3) succeeded with fewer iterations, whereas stateful operations like register counting (Experiments 4, 7) failed without precise instructions.
    \item \textbf{Compiler Feedback Has Limits}: The feedback loop corrected syntactic issues (e.g., one error in Experiment 1 after five iterations), but could not overcome semantic gaps in general prompts, as seen in the persistent failures of Experiments 4 and 7.
    \item \textbf{Advanced Features Need Explicit Guidance}: Register-based logic succeeded only with detailed syntax and scoping (Experiment 5), unlike header parsing or forwarding, which worked with minimal prompts (Experiments 2, 3).
    \item \textbf{Scope-Based Prompting Enhances Reliability}:
    Inspired by the context-driven design separation proposed in the NADA framework~\cite{he2024nada}, we structured prompts around specific functionalities (e.g., parsing, modification). This led to improved outcomes across intents within the same domain—as demonstrated by the success of Experiment~6, which converged in an average of 4.67 iterations.
    

\end{itemize}

These findings validate the hypothesis that effective prompt engineering in intent-based networking hinges on aligning prompts with the functional scope of the intent. General prompts suffice for basic tasks, but moderately complex intents demand tailored, domain-specific instructions. Future enhancements could leverage intent classifiers or scope mappers to automate prompt selection, streamlining the translation process.

\section{Conclusion and Future Work}

This project explored how Large Language Models, specifically GPT-4, can be used to generate P4\_16 code from high-level natural language intents. We implemented a closed-loop system that iteratively refines the code based on compiler feedback until a valid program is produced. Through a series of experiments, we found that while general prompts work for simple parsing and forwarding tasks, scope-specific prompt templates significantly improve success rates for moderately complex intents such as header modification or register usage.

Inspired by the NADA framework~\cite{he2024nada}, we adopted a scope-based approach to prompt generation and demonstrated its effectiveness across a variety of networking use cases. Our findings suggest that LLMs have the potential to assist in intent-to-configuration translation, but reliability depends heavily on how well the prompt aligns with the underlying intent.

While the findings are promising, they are based on a limited set of nine experiments and should be interpreted as exploratory rather than conclusive. Further large-scale testing is required to strengthen the statistical validity of the results.

Future work could focus on several directions. One possibility is to develop a classifier that automatically identifies the intent scope and selects the appropriate prompt template, streamlining the translation process. Another direction involves conducting additional experiments to analyze GPT-4’s behavior in generating P4\_16 code, identifying patterns in its successes and failures (e.g., error types, iteration counts); these patterns could be used to train a machine learning model to optimize prompt generation, improving the system’s efficiency and success rate. Additionally, fine-tuning LLMs on a corpus of syntactically valid P4\_16 code could enhance reliability, particularly for complex intents like stateful operations. Finally, integrating formal verification or behavioral testing into the feedback loop may enable not just syntactic correctness, but also semantic validation, ensuring the generated code accurately implements the intended behavior.

\section{Github Code}
Source Code:
\url{https://github.com/titanium-exe/cmpt415/tree/main}


\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}
\end{document}